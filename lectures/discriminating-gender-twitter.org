#+TITLE: Discriminating Gender on Twitter
#+AUTHOR: David Arroyo Menéndez
#+OPTIONS: H:2 toc:nil num:t
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation]
#+BEAMER_THEME: Madrid
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)

* Summary
** Presentation

Discriminating Gender on Twitter
John D. BurgerThe MITRE Corporation, Bedford, Massachusetts
John HendersonThe MITRE Corporation, Bedford, Massachusetts
George KimThe MITRE Corporation, Bedford, Massachusetts
Guido ZarrellaThe MITRE Corporation, Bedford, Massachusetts

URL: https://dl.acm.org/ft_gateway.cfm?id=2145568&ftid=1146137&dwn=1&CFID=96718218&CFTOKEN=5ad99f87ffded6b-6BA0215F-CA4F-5037-02956322E689CADF
Citations: 575
Published on: EMNLP '11 Proceedings of the Conference on Empirical Methods in Natural Language Processing

** Abstract

*** Social Value
Accurate prediction of demographic attributes from social media and
other informal online content is valuable for marketing,
personalization, and legal investigation. 

*** Brief paper description
This paper describes the construction of a large, multilingual dataset
labeled with gender, and investigates statistical models for
determining the gender of uncharacterized Twitter users. 

*** Technical Solution and Advance in Science
We explore several different classifier types on this dataset. We show
the degree to which classifier accuracy varies based on tweet volumes
as well as when various kinds of profile metadata are included in the
models. We also perform a large-scale human assessment using Amazon
Mechanical Turk. Our methods significantly out-perform both baseline
models and almost all humans on the same task.

** Introduction

*** Background:
The rapid growth of social media in recent years, exemplified
by Facebook and Twitter, has led to a massive
volume of user-generated informal text. This in turn has
sparked a great deal of research interest in aspects of social
media, including automatically identifying latent demographic
features of online users. 

*** Gap
Many latent features
have been explored, but gender and age have generated
great interest (Schler et al., 2006; Burger and Henderson,
2006; Argamon et al., 2007; Mukherjee and Liu, 2010;
Rao et al., 2010). Accurate prediction of these features
would be useful for marketing and personalization concerns,
as well as for legal investigation.

*** Technical solution

In this work, we investigate the development of highperformance
classifiers for identifying the gender of
Twitter users. We cast gender identification as the obvious
binary classification problem, and explore the use
of a number of text-based features.

*** Article structure explained

In Section 2, we describe our Twitter corpus, and our methods for
labeling a large subset of this data for gender. In Section 3 we
discuss the features that are used in our classifiers. We describe our
Experiments in Section 4, including our exploration of several
different classifier types. In Section 5 we present and analyze
performance results, and discuss some directions for acquiring
additional data by simple self-training techniques. Finally in Section
6 we summarize our findings, and describe extensions to the work that
we are currently exploring.

** Data

*** Twitter Description 

Twitter is a social networking and micro-blogging platform
whose users publish short messages or tweets. In
late 2010, it was estimated that Twitter had 175 million
registered users worldwide, producing 65 million tweets
per day (Miller, 2010). Twitter is an attractive venue
for research into social media because of its large volume,
diverse and multilingual population, and the generous
nature of its Terms of Service. This has led many researchers
to build corpora of Twitter data (Petrovic et al.,
2010; Eisenstein et al., 2010). In April 2009, we began
sampling data from Twitter using their API at a rate of
approximately 400,000 tweets per day. This represented
approximately 2% of Twitter’s daily volume at the time,
but this fraction has steadily decreased to less than 1% by
2011. This decrease is because we sample roughly the
same number of tweets every day while Twitter’s overall
volume has increased markedly. Our corpus thus far contains
approximately 213 million tweets from 18.5 million
users, in many different languages

*** Fields of interest

+ Screen name (e.g., jsmith92, kingofpittsburgh)
+ Full name (e.g., John Smith, King of Pittsburgh)
+ Location (e.g., Earth, Paris)
+ URL (e.g., the user’s web site, Facebook page, etc.)
+ Description (e.g., Retired accountant and grandfather)

*** Using Dataset

[...]
We partitioned our dataset by user into three distinct
subsets, training, development, and test, with sizes as indicated
in Figure 1. That is, all the tweets from each user
are in a single one of the three subsets. This is the corpus
we use in the remainder of this paper.
[...]

** Experiments

[...]
The sheer volume of data presents a challenge for many of the
available machine learning toolkits, e.g. WEKA (Hall et al., 2009) or
MALLET (McCallum, 2002)
[...]

[...]  We performed initial feasibility experiments using a wide
variety of different classifier types, including Support Vector
Machines, Naive Bayes, and Balanced Winnow [...]


*** Field combinations

We performed a number of experiments with the Winnow
algorithm described above. We trained it on the training
set and evaluated on the development set for each of
the four user fields in isolation, as well as various combinations,
in order to simulate different use cases for systems
that perform gender prediction from social media
sources. [...]

*** Human performance

We wished to compare our classifier’s efficacy to human
performance on the same task. A number of researchers
have recently experimented with the use of Amazon Mechanical
Turk (AMT) to create and evaluate human language
data (Callison-Burch and Dredze, 2010). AMT
and other crowd-sourcing platforms allow simple tasks to
be posted online for large numbers of anonymous workers
to complete.
[...]

*** Self-training

Our final experiments were focused on exploring the use
of unlabeled data, of which we have a great deal. We
performed some initial experiments on a self-training approach
to labeling more data. We trained the all-fields
classifier on half of our training data, and applied it to the
other half. We trained a new classifier on this full training
set, which now included label errors introduced by the
limitations of the first classifier. This provided a simulation
of a self-training setup using half the training data.
Any robust gains due to self-training should be revealed
by this setup.

** Results

*** Field combinations

*** Human performance

*** Self-training

** Conclusion

*** What 
In this paper, we have presented several configurations of
a language-independent classifier for predicting the gender
of Twitter users. The large dataset used for construction
and evaluation of these classifiers was drawn from
Twitter users who also completed blog profile pages.

*** Technical result (percentages)
These classifiers were tested on the largest set of
gender-tagged tweets to date that we are aware of. The
best classifier performed at 92% accuracy, and the classifier
relying only on tweet texts performed at 76% accuracy.
Human performance was assessed on this latter
condition, and only 5% of 130 humans performed 100 or
more classifications with higher accuracy than this machine.

*** Future Work
In future work, we will explore how well such models
carry over to gender identification in other informal online
genres such as chat and forum comments. Furthermore,
we have been able to assign demographic features
beside gender, including age and location, to our Twitter
dataset. We have begun to build classifiers for these
features as well.
